MMU
===

Virtual addresses are on 41 bits for kvx when using 64bit mode.
To differentiate kernel from user space, we use the high order bit
(bit 40). if 1 , the higher remaining bits must also be set to one
and it is kernel space. The same applied for 0 and it's user space
mapping.

Memory Map
==========

In Linux physical memories are arranged into banks according to the cost of an
access in term of distance to a memory. As we are UMA architecture we only have
one bank and thus one node.

A node is divided into several kind of zone. For example if DMA can only access
a specific area in the physical memory we will define a ZONE_DMA for this purpose.
In our case we are considering that DMA can access all DDR so we don't have a specific
zone for this. On 64 bit architecture all DDR can be mapped in virtual kernel space
so there is no need for a ZONE_HIGHMEM. That means that in our case there is
only one ZONE_NORMAL. This will be updated if DMA cannot access all memory.

Currently, the memory mapping is the following for 4KB page:

+-----------------------+-----------------------+------+-------+--------------+
| Start                 | End                   | Attr | Size  | Name         |
+-----------------------+-----------------------+------+-------+--------------+
| 0000 0000 0000 0000   | 0000 003F FFFF FFFF   | ---  | 256GB | User         |
| 0000 0040 0000 0000   | 0000 007F FFFF FFFF   | ---  | 256GB |   MMAP       |
| 0000 0080 0000 0000   | FFFF FF7F FFFF FFFF   | ---  | ---   | Gap          |
| FFFF FF80 0000 0000   | FFFF FFFF FFFF FFFF   | ---  | 512GB | Kernel       |
|   FFFF FF80 0000 0000 |   FFFF FF8F FFFF FFFF | RWX  | 64GB  |   Direct Map |
|   FFFF FF90 0000 0000 |   FFFF FF90 3FFF FFFF | RWX  | 1GB   |   Vmalloc    |
|   FFFF FF90 4000 0000 |   FFFF FFFF FFFF FFFF | RW   | 447GB |   Free area  |
+-----------------------+-----------------------+------+-------+--------------+

Enable the MMU
==============

All kernel functions and symbols are in virtual memory except for kvx_start()
function which is loaded at 0x0 in physical memory.
To be able to switch from physical addresses to virtual addresses we choose to
setup the TLB at the very beginning of the boot process to be able to map both
pieces of code. For this we added two entries in the LTLB. The first one,
LTLB[0], contains the mapping between virtual memory and DDR. Its size is 512MB.
The second entry, LTLB[1], contains a flat mapping of the first 2MB of the SMEM.
Once those two entries are present we can enable the MMU. LTLB[1] will be
removed during paging_init() because once we are really running in virtual space
it will not be used anymore.
In order to access more than 512MB DDR memory, the remaining memory (> 512MB) is
refill using a comparison in kernel_perf_refill that does not walk the kernel
page table, thus having a faster refill time for kernel. These entries are
inserted into the LTLB for easier computation (4 LTLB entries). The drawback of
this approach is that mapped entries are using RWX protection attributes,
leading to no protection at all.

Kernel strict RWX
=================

CONFIG_STRICT_KERNEL_RWX is enabled by default in default_defconfig.
Once booted, if CONFIG_STRICT_KERNEL_RWX is enable, the kernel text and memory
will be mapped in the init_mm page table. Once mapped, the refill routine for
the kernel is patched to always do a page table walk, bypassing the faster
comparison but enforcing page protection attributes when refilling.
Finally, the LTLB[0] entry is replaced by a 4K one, mapping only exceptions with
RX protection. It allows us to never trigger nomapping on nomapping refill
routine which would (obviously) not work... Once this is done, we can flush the
4 LTLB entries for kernel refill in order to be sure there is no stalled
entries and that new entries inserted in JTLB will apply.

By default, the following policy is applied on vmlinux sections:
- init_data: RW
- init_text: RX (or RWX if parameter rodata=off)
- text: RX (or RWX if parameter rodata=off)
- rodata: RW before init, RO after init
- sdata: RW

Kernel RWX mode can then be switched on/off using /sys/kvx/kernel_rwx file.

Privilege Level
================
Since we are using privilege levels on kvx, we make use of the virtual
spaces to be in the same space as the user. The kernel will have the
$ps.mmup set in kernel (PL1) and unset for user (PL2).
As said in kvx documentation, we have two cases when the kernel is
booted:
- Either we have been booted by someone (bootloader, hypervisor, etc)
- Or we are alone (boot from flash)

In both cases, we will use the virtual space 0. Indeed, if we are alone
on the core, then it means nobody is using the MMU and we can take the
first virtual space. If not alone, then when writing an entry to the tlb
using writetlb instruction, the hypervisor will catch it and change the
virtual space accordingly.

Memblock
========

When the kernel starts there is no memory allocator available. One of the first
step in the kernel is to detect the amount of DDR available by getting this
information in the device tree and initialize the low-level "memblock" allocator.

We start by reserving memory for the whole kernel. On the ISS we can see that
512Mo of DDR are managed by the memory allocator and some physical memory is
reserved for the kernel:

setup_bootmem: Memory  : 0x100000000 - 0x120000000
setup_bootmem: Reserved: 0x10001f000 - 0x1002d1bc0

During the paging init we need to set:
  - min_low_pfn that is the lowest PFN available in the system
  - max_low_pfn that indicates the end if NORMAL zone
  - max_pfn that is the number of pages in the system

This setting is used for dividing memory into pages and for configuring the
zone. See the memory map section for more information about ZONE.

Zones are configured in free_area_init_core(). During start_kernel() other
allocations are done for command line, cpu areas, PID hash table, different
caches for VFS. This allocator is used until mem_init() is called.

mem_init() is provided by the architecture. For MPPA we just call
free_all_bootmem() that will go through all pages that are not used by the
low level allocator and mark them as not used. So physical pages that are
reserved for the kernel are still used and remain in physical memory. All pages
released will now be used by the buddy allocator.

Peripherals
===========

Peripherals are mapped using standard ioremap infrastructure and hence
mapped addresses are located in the vmalloc space.

LTLB Usage
==========

LTLB is used to add resident mapping which allows for faster MMU lookup.
Currently, the LTLB is used to map some mandatory kernel pages and to allow fast
accesses to l2 cache (mailbox and registers).
When CONFIG_STRICT_KERNEL_RWX is disabled, 4 entries are reserved for kernel
TLB refill using 512MB pages. When CONFIG_STRICT_KERNEL_RWX is enabled, these
entries are unused since kernel is paginated using the same mecanism than for
user (page walking and entries in JTLB)

Page Table
==========

We support three levels for the page table and 4KB for page size.

3 levels page table
-------------------

...-----+--------+--------+--------+--------+--------+
      40|39    32|31    24|23    16|15     8|7      0|
...-----++-------+--+-----+---+----+----+---+--------+
         |          |         |         |
         |          |         |         +--->  [11:0] Offset (12 bits)
         |          |         +------------->  [20:12] PTE offset (9 bits)
         |          +----------------------->  [29:21] PMD offset (9 bits)
         +---------------------------------->  [39:30] PGD offset (10 bits)
Bits 40 to 64 are signed extended according to bit 39. If bit 39 is equal to 1
we are in kernel space.

As 10 bits are used for PGD we need to allocate 2 pages.

4 levels page table
-------------------

The problem with the 4 levels page table is that it will require more memory
accesses. As the page table walk is fully software we want to reduce the number
of load and store to memory so we will avoid to use it. Of course it's even more
true for a 5 level page table.

Conclusion:

Today we are only supporting 4Ko page size. If we want support page of 64Ko we
will need to use a layout like this one:

...---+--------+--------+--------+--------+--------+
    40|39    32|31    24|23    16|15     8|7      0|
...--++------+-+----+---+--------++-------+--------+
     |       |      |             |
     |       |      |             +------->  [15:0] Offset (16 bits)
     |       |      +--------------------->  [27:16] PTE offset (12 bits)
     |       +---------------------------->  [33:28] PMD offset (6 bits)
     +------------------------------------>  [40:34] PGD offset (7 bits)
Bits 41 to 64 are signed extended according to bit 40. If bit 40 is equal to 1
we are dealing with kernel addresses. As we can see we are loosing memory space
so a better layout could be to use a 2 level page table. It will also reduce
the number of load and store in memory.

PTE format
==========

About the format of the PTE entry, as we are not forced by hardware for choices,
we choose to follow the format described in the RiscV implementation as a
starting point.

 +---------+--------+----+--------+---+---+---+---+---+---+------+---+---+
 | 63..23  | 22..13 | 12 | 11..10 | 9 | 8 | 7 | 6 | 5 | 4 | 3..2 | 1 | 0 |
 +---------+--------+----+--------+---+---+---+---+---+---+------+---+---+
     PFN     Unused   S    PageSZ   H   G   X   W   R   D    CP    A   P
       where:
        P: Present
        A: Accessed
        CP: Cache policy
        D: Dirty
        R: Read
        W: Write
        X: Executable
        G: Global
        H: Huge page
        PageSZ: Page size as set in TLB format (0:4Ko, 1:64Ko, 2:2Mo, 3:512Mo)
        S: Soft/Special
        PFN: Page frame number (depends on page size)

Huge bit must be somewhere in the first 12 bits to be able to detect it
when reading the PMD entry.

PageSZ must be on bit 10 and 11 because it matches the TEL.PS bits. And
by doing that it is easier in assembly to set the TEL.PS to PageSZ.

Fast TLB refill
===============

KVX core does not feature a hardware page walker. This work must be done
by the core in software. In order to optimize TLB refill, a special fast
path is taken when entering in kernel space.
In order to speed up the process, the following actions are taken:
# Save some registers in a per process scratchpad
# If the trap is a nomapping then try the fastpath
# Save some more registers for this fastpath
# Check if faulting address is a memory direct mapping one.
 # If entry is a direct mapping one and RWX is not enabled, add an entry into LTLB
 # If not, continue
# Try to walk the page table
 # If entry is not present, take the slowpath (do_page_fault)
# Refill the tlb properly
# Exit by restoring only a few registers

ASN Handling
============

Disclaimer: Some part of this are taken from ARC architecture.

KVX MMU provides 9-bit ASN (Address Space Number) in order to tag TLB entries.
It allows for multiple process with the same virtual space to cohabit without
the need to flush TLB everytime we context switch.
kvx implementation to use them is based on other architectures (such as arc
or xtensa) and uses a wrapping ASN counter containing both cycle/generation and
asn.

+---------+--------+
|63     10|9      0|
+---------+--------+
  Cycle      ASN

This ASN counter is incremented monotonously to allocate new ASNs. When the
counter reaches 511 (9 bit), TLB is completely flushed and a new cycle is
started. A new allocation cycle, post rollover, could potentially reassign an
ASN to a different task. Thus the rule is to reassign an ASN when the current
context cycles does not match the allocation cycle.
The 64 bit @cpu_asn_cache (and mm->asn) have 9 bits MMU ASN and rest 55 bits
serve as cycle/generation indicator and natural 64 bit unsigned math
automagically increments the generation when lower 9 bits rollover.
When the counter completely wraps, we reset the counter to first cycle value
(ie cycle = 1). This allows to distinguish context without any ASN and old cycle
generated value with the same operation (XOR on cycle).

Huge page
=========

Currently only 3 level page table has been implemented for 4Ko base page size.
So the page shift is 12 bits, the pmd shift is 21 and the pgdir shift is 30
bits. This choice implies that for 4Ko base page size if we use a PMD as a huge
page the size will be 2Mo and if we use a PUD as a huge page it will be 1Go.

To support other huge page sizes (64Ko and 512Mo) we need to use several
contiguous entries in the page table. For huge page of 64Ko we will need to
use 16 entries in the PTE and for a huge page of 512Mo it means that 256
entries in PMD will be used.

Debug
=====

In order to debug the page table and tlb entries, gdb scripts contains commands
which allows to dump the page table:
- lx-kvx-page-table-walk
 - Display the current process page table by default
- lx-kvx-tlb-decode
 - Display the content of $tel and $teh into something readable

Other commands available in kvx-gdb are the following:
- mppa-dump-tlb
 - Display the content of TLBs (JTLB and LTLB)
- mppa-lookup-addr
 - Find physical address matching a virtual one
